{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "import os\n",
    "import numpy as np \n",
    "import torch \n",
    "import glob \n",
    "import torch.nn as nn \n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam \n",
    "from torch.autograd import Variable\n",
    "import torchvision \n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cuda available \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 , 0.5 , 0.5 ],[0.5 , 0.5 , 0.5 ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory paths \n",
    "base_dir = \"basedata\"\n",
    "training = \"training\"\n",
    "validationString = \"validation\"\n",
    "\n",
    "# training ,validation locations \n",
    "training_path = os.path.join(base_dir,training)\n",
    "test_path = os.path.join(base_dir,\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader \n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(training_path , transform = transformer),\n",
    "    batch_size = 12 , shuffle= True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path , transform = transformer),\n",
    "    batch_size = 12 , shuffle= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class\n",
    "classes = os.listdir(training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Neural Net \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes = 41):\n",
    "        super(ConvNet,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels= 3 , out_channels=12 , kernel_size= 3, stride = 1 , padding = 1)\n",
    "         \n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 12)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size= 2)\n",
    "        # \n",
    "        self.conv2 = nn.Conv2d(in_channels = 12 , out_channels=20 , kernel_size= 3 , stride=1 , padding= 1)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels= 20 , out_channels= 32 , kernel_size = 3 , stride = 1 , padding = 1)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(num_features = 32)\n",
    "\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(in_features= 32*150*150 , out_features= num_classes)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        # print(input.shape)\n",
    "        output = self.conv1(input)\n",
    "        # print(output.shape)\n",
    "        output = self.bn1(output)\n",
    "        # print(output.shape)\n",
    "        output = self.relu1(output)\n",
    "        # print(output.shape)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        # print(output.shape)\n",
    "        output = self.relu2(output)\n",
    "        # print(output.shape)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        # print(output.shape)\n",
    "        output = self.bn3(output)\n",
    "        # print(output.shape)\n",
    "        output = self.relu3(output)\n",
    "        # print(output.shape)\n",
    "\n",
    "        # resizing \n",
    "        output  = output.view(-1,32*150*150)\n",
    "        # print(output.shape)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=41).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function \n",
    "optimizer = Adam(model.parameters(),lr = 0.001 , weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate testing_training , size \n",
    "train_count = len(glob.glob(training_path+'/**/*.JPG'))\n",
    "test_count = len(glob.glob(test_path+'/**/*.JPG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0Train Loss : 3412Train Accuracy : 0\n",
      "Epoch : 1Train Loss : 1349Train Accuracy : 0\n",
      "Epoch : 2Train Loss : 1060Train Accuracy : 0\n",
      "Epoch : 3Train Loss : 660Train Accuracy : 0\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0 \n",
    "    # print('Epoch : ' + str(epoch)+'Train Loss : ' + str(int(train_loss))+ 'Train Accuracy : '+str(int(train_accuracy)))\n",
    "    for i , (images,labels) in enumerate(train_loader):\n",
    "        if(torch.cuda.is_available()):\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs , labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _ , prediction = torch.max(outputs.data , 1)\n",
    "\n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "    train_accuracy  = train_accuracy/train_count\n",
    "\n",
    "    model.eval()\n",
    "    print('Epoch : ' + str(epoch)+'Train Loss : ' + str(int(train_loss))+ 'Train Accuracy : '+str(int(train_accuracy)))\n",
    "\n",
    "    # test_accuracy = 0.0\n",
    "    # for i , (images,labels) in enumerate(test_loader):\n",
    "    #     if(torch.cuda.is_available()):\n",
    "    #         images = Variable(images.cuda())\n",
    "    #         labels = Variable(labels.cuda())\n",
    "\n",
    "    #     outputs = model(images)\n",
    "    #     loss = loss_function(outputs , labels)\n",
    "    #     _ , prediction = torch.max(outputs.data , 1)\n",
    "\n",
    "    #     test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    # test_accuracy  = test_accuracy/train_count\n",
    "    \n",
    "    # print('Epoch : ' + str(epoch)+'Train Loss : ' + str(int(train_loss))+ 'Train Accuracy : '+str(int(train_accuracy)))\n",
    "\n",
    "    # if test_accuracy > best_accuracy:\n",
    "    #     torch.save(model.state_dict(),'best_checlpoint_model')\n",
    "    #     best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd50aa143d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_Fpath = os.path.join(training_path,os.listdir(training_path)[0])\n",
    "img_path = os.path.join(image_Fpath,os.listdir(image_Fpath)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img1 = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transformer1 = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = transformer1(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 150, 150])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = torch.randn(256, 32, 75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 32, 75, 75])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 180000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_trial  = trial.view(-1,32*75*75)\n",
    "new_trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6abc19a8e1fc5a4ded62ce588c026b0071d26d608f7dba32576a5a9967215991"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
